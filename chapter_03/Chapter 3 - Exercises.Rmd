---
title: "Chapter 3 - Exercises"
output:
  html_document:
    df_print: paged
  html_notebook:
    number_sections: yes
    theme: cerulean
---

# Chapter 3 - Classification with k-nearest neighbors

```{r}
# import libraries from 3.2
#install.packages("mlr", dependencies = TRUE)
#install.packages("tidyverse", dependencies = TRUE)
#install.packages("mclust", dependencies = TRUE)
library(mlr)
library(tidyverse)

# Listing 3.1 (test library functions)
data(diabetes, package="mclust")
diabetesTib <- as_tibble(diabetes)

summary(diabetesTib)
head(diabetesTib, n = 5)
```
```{r}
# Listing 3.2
ggplot(diabetesTib, aes(glucose, insulin, col = class)) + 
    geom_point() +
    theme_bw()

ggplot(diabetesTib, aes(sspg, insulin, col = class)) +
    geom_point() +
    theme_bw()

ggplot(diabetesTib, aes(sspg, glucose, col = class)) +
    geom_point() +
    theme_bw()
```
## Exercise 1

```{r}
# Exercise 1
ggplot(diabetesTib, aes(glucose, insulin, shape = class)) + 
    geom_point() +
    theme_bw()

ggplot(diabetesTib, aes(glucose, insulin, col = class, shape = class)) + 
    geom_point() +
    theme_bw()
```
```{r}
# Section 3.2.3
diabetesTask <- makeClassifTask(data = diabetesTib, target = "class")
print(diabetesTask)

knn <- makeLearner("classif.knn", par.vals = list ("k" = 2))
knnModel <- train(knn, diabetesTask)
knnPredictions <- predict(knnModel, newdata = diabetesTib)

knnPerf <- performance(knnPredictions, measures = list(mmce, acc))
knnPerf
```

```{r}
# Section 3.5
holdout <- makeResampleDesc(method = "Holdout", split = 2/3, stratify = TRUE)
holdoutCV <- resample(learner = knn, task = diabetesTask, 
                      resampling = holdout, measures = list(mmce, acc))

holdoutCV$aggr
```

# Exercise 2

```{r}
holdout2 <- makeResampleDesc(method = "Holdout", split = 90/100, stratify = FALSE)
holdoutCV2 <- resample(learner = knn, task = diabetesTask,
                       resampling = holdout2, measures = list(mmce, acc))

holdoutCV2$aggr
```

```{r, message = FALSE, echo = TRUE}
# Section 3.5.1
calculateConfusionMatrix(holdoutCV$pred, relative = TRUE)
calculateConfusionMatrix(holdoutCV2$pred, relative = TRUE)

# Section 3.5.2
kFold <- makeResampleDesc(method = "RepCV", folds = 10, reps = 50, stratify = TRUE)
kFoldCV <- resample(learner = knn, task = diabetesTask, resampling = kFold, 
                    measures = list(mmce, acc))
```

```{r, message = TRUE, echo = TRUE}
kFoldCV$aggr
```

# Exercise 3

```{r, message = FALSE, echo = TRUE, results = 'hide'}
kFoldResults <- tibble(run = integer(), Method = character(), Folds = integer(), 
                           Reps = integer(), mmce.test.mean = double(), acc.test.mean = double())

for (i in c(1, 2, 3, 4, 5)) {
  kFold2 <- makeResampleDesc(method = "RepCV", folds = 3, reps = 5, stratify = TRUE)
  kFoldCV2 <- resample(learner = knn, task = diabetesTask, resampling = kFold2, 
                    measures = list(mmce, acc))
  
  kFoldResults <- add_row(kFoldResults, run = i, Method = "RepCV", Folds = 3, Reps = 5, 
                 mmce.test.mean = kFoldCV2$aggr['mmce.test.mean'],
                 acc.test.mean = kFoldCV2$aggr['acc.test.mean'])

  kFold3 <- makeResampleDesc(method = "RepCV", folds = 3, reps = 500, stratify = TRUE)
  kFoldCV3 <- resample(learner = knn, task = diabetesTask, resampling = kFold3, 
                    measures = list(mmce, acc))

  kFoldResults <- add_row(kFoldResults, run = i, Method = "RepCV", Folds = 3, Reps = 500, 
                 mmce.test.mean = kFoldCV3$aggr['mmce.test.mean'],
                 acc.test.mean = kFoldCV3$aggr['acc.test.mean'])
}

tally <- kFoldResults %>% 
  group_by(Method, Folds, Reps) %>%
  summarize(mmce.mean = mean(mmce.test.mean),
            acc.mean = mean(acc.test.mean),
            count = n())

```

```{r, echo = TRUE, message = TRUE, results = 'show'}
tally
```

## Confusion matrices (last iteration of each)
```{r}

# folds = 10, reps = 50, stratify = TRUE
calculateConfusionMatrix(kFoldCV$pred, relative = TRUE)

# folds = 3, reps = 5, stratify = TRUE
calculateConfusionMatrix(kFoldCV2$pred, relative = TRUE)

# folds = 3, reps = 500, stratify = TRUE
calculateConfusionMatrix(kFoldCV3$pred, relative = TRUE)
```

# Exercise 4
```{r}
# these methods are invalid, LOO does not "fold" nor does it "repeat"
# LOO2 <- makeResampleDesc(method = "LOO", stratify = TRUE)
# LOO3 <- makeResampleDesc(method = "LOO", reps = 5)
```

# Section 3.7
```{r, message = FALSE}
knnParamSpace <- makeParamSet(makeDiscreteParam("k", values = 1:10))
gridSearch <- makeTuneControlGrid()
cvForTuning <- makeResampleDesc("RepCV", folds = 10, reps = 20)
tunedK <- tuneParams("classif.knn", task = diabetesTask, 
                     resampling = cvForTuning,
                     par.set = knnParamSpace, control = gridSearch)

tunedK
```

```{r}
knnTuningData <- generateHyperParsEffectData(tunedK)
plotHyperParsEffect(knnTuningData, x = "k", y = "mmce.test.mean", plot.type = "line") +
  theme_bw()
```
```{r}
tunedKnn <- setHyperPars(makeLearner("classif.knn"), par.vals = tunedK$x)
tunedKnnModel <- train(tunedKnn, diabetesTask)
```

# Section 3.7.1 - hyperparameter tuning with nested cross-validation
```{r, message = FALSE}
inner <- makeResampleDesc("CV")
outer <- makeResampleDesc("RepCV", folds = 10, reps = 5)
knnWrapper <- makeTuneWrapper("classif.knn", resampling = inner,
                              par.set = knnParamSpace, control = gridSearch)
cvWithTuning <- resample(knnWrapper, diabetesTask, resampling = outer)
cvWithTuning
```
# Section 3.7.2 - make predictions
```{r}
newDiabetesPatients <- tibble(glucose = c(82, 108, 300),
                              insulin = c(361, 288, 1052),
                              sspg = c(200, 186, 135))
newDiabetesPatients
newPatentisPredictions <- predict(tunedKnnModel, newdata = newDiabetesPatients)
getPredictionResponse(newPatentisPredictions)

```
# Exercise 5
```{r, message = FALSE}
# load and summarize iris data
irisTib <- as_tibble(iris)
summary(irisTib)
head(irisTib, n = 5)

# visualize data
ggplot(irisTib, aes(Sepal.Length, Sepal.Width, col = Species)) + 
    geom_point() +
    theme_bw()

ggplot(irisTib, aes(Petal.Length, Petal.Width, col = Species)) + 
    geom_point() +
    theme_bw()

knnParamSpaceIris5 <- makeParamSet(makeDiscreteParam("k", values = 1:10))
irisTask5 <- makeClassifTask(data = irisTib, target = "Species")

gridSearchIris5 <- makeTuneControlGrid()
cvForTuningIris5 <- makeResampleDesc("RepCV", folds = 10, reps = 20)
tunedKIris5 <- tuneParams("classif.knn", task = irisTask5, 
                     resampling = cvForTuningIris5,
                     par.set = knnParamSpaceIris5, control = gridSearchIris5)

tunedKIris5
knnTuningDataIris5 <- generateHyperParsEffectData(tunedKIris5)
plotHyperParsEffect(knnTuningDataIris5, x = "k", y = "mmce.test.mean", plot.type = "line") +
  theme_bw()
```
# Exercise 6
```{r, message = FALSE}
# set up inner and outer loops for hyperparameter tuning
irisTask6 <- makeClassifTask(data = irisTib, target = "Species")
knnParamSpaceIris6 <- makeParamSet(makeDiscreteParam("k", values = 1:5))
gridSearchIris6 <- makeTuneControlGrid()

innerIris6 <- makeResampleDesc("CV")
outerIris6 <- makeResampleDesc(method = "Holdout", split = 2/3, stratify = TRUE)
knnWrapperIris6 <- makeTuneWrapper("classif.knn", resampling = innerIris6,
                              par.set = knnParamSpaceIris6, control = gridSearchIris6)
cvIrisWithTuning6 <- resample(knnWrapperIris6, irisTask6, resampling = outerIris6)
cvIrisWithTuning6
```
# Exercise 7
```{r, message = FALSE}
# set up inner and outer loops for hyperparameter tuning
irisTask7 <- makeClassifTask(data = irisTib, target = "Species")
knnParamSpaceIris7 <- makeParamSet(makeDiscreteParam("k", values = 1:5))
gridSearchIris7 <- makeTuneControlGrid()

innerIris7 <- makeResampleDesc("CV")
outerIris7 <- makeResampleDesc("RepCV", folds = 5)
knnWrapperIris7 <- makeTuneWrapper("classif.knn", resampling = innerIris7,
                              par.set = knnParamSpaceIris7, control = gridSearchIris7)
cvIrisWithTuning7 <- resample(knnWrapperIris7, irisTask7, resampling = outerIris7)
cvIrisWithTuning7
```
